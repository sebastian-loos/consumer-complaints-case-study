---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup_chunks, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "images/",
  out.width = "100%",
  message = FALSE,
  warning = FALSE
)
```

## Load Packages

```{r}
library(readr)
library(here)
library(tidyverse)
library(tidytext)
library(tidymodels)
library(SnowballC)
library(tm)
library(rsample)
library(recipes)
library(rpart)
library(tune)
library(vip)
library(workflows)
```

## Load data sets

```{r}
complaints_train <- read_csv(here::here("data", "raw_data", "data_complaints_train.csv")) |> 
  janitor::clean_names()

# organize variable names and types
complaints_train <- complaints_train |> 
  select(-submitted_via) |> 
  mutate(complaint_id = as.factor(row_number()), .before = everything(),
         product = as.factor(product)) |> 
  rename(complaint = consumer_complaint_narrative)
complaints_train

complaints_test <- read_csv(here::here("data", "raw_data", "data_complaints_test.csv")) |>
  janitor::clean_names() |> 
  select(-submitted_via)
complaints_test

# save(complaints_train, complaints_test, file = here::here("data", "tidy_data", "project_5.rda"))
```

## Explore data

```{r}
head(complaints_train)
glimpse(complaints_train)
head(complaints_test)
glimpse(complaints_test)

skimr::skim(complaints_train)
skimr::skim(complaints_test)
```

For the training data we have initially 90'975 complaints (rows) and 6 total variables (columns) where no values are missing. We have 4 different products for which the distribution can be seen in the following plot.

```{r}
complaints_train |> 
  group_by(product) |> 
  summarise(count = n()) |> 
  mutate(percent = count / sum(count) * 100 ) |>  
  ungroup() |>  
  arrange(desc(count)) |> 
  ggplot(aes(x = "", y = percent, fill = product)) +
  geom_col(color = "black") +
  geom_text(aes(label = round(percent, digits = 1)),
            position = position_stack(vjust = 0.5)) +
  coord_polar(theta = "y") +
  scale_fill_brewer() +
  labs(title = "Distribution of products for the training dataset.",
       x = "",
       y = "Percentage of Product Type") +
  guides(fill = guide_legend(title = "Product")) +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        panel.background = element_rect(fill = "#ebf2ff"),
        plot.background = element_rect(fill = "#ebf2ff"),
        legend.background = element_rect(fill = "#ebf2ff"))

```

There is no clear tendency visible between the lenght of a complaint and the product type.
```{r}
complaints_train |> 
  mutate(textlength = nchar(complaint), .after = complaint) |> 
  ggplot(mapping = aes(x = textlength)) +
  geom_histogram(binwidth = 200) +
  facet_grid(product ~ .)
  
complaints_train |> 
  mutate(textlength = nchar(complaint), .after = complaint) |> 
  ggplot(mapping = aes(x = textlength, product)) +
  geom_boxplot(outlier.shape = NA) +
  scale_x_continuous(limits = c(0, 3500))

```

```{r}
# complaints_train |> distinct(state) |> view()
```

The variable state has 62 unique values which include all the 50 US states and other territories outside the Union as Puerto Rico (PR) or Armed forces Pacific (AP).

## Term frequency

From the exploring of the data, no correlation between the different variables were visible. The most promising way to design the model is to create a matrix with how frequent a given word appears for a given complaint and use it as features for a machine learning algorithm. In the following the matrix is generated.

### Data preparation

####Reducing Sample size
```{r}
# reduce sample size
# complaints_train <- complaints_train |>
#   sample_n(10000)
```

#### Create tidy data set

```{r}
complaints_train_tidy <- complaints_train |> 
  mutate(
    # lower case
    complaint = str_to_lower(complaint),
    
    # remove any strings such as "XX", "XXX", "XXXX" in the complaints
    complaint = str_replace_all(complaint, "xx+", "")) |> 
  
   # remove digits and punctuation
  mutate(
    complaint = removePunctuation(complaint),
    complaint = stripWhitespace(complaint),
    complaint = str_replace_all(complaint,'[0-9]+', ""),
    complaint = str_trim(complaint)
  )
```


### Variation 1

```{r}
# complaints_corpus1 <- Corpus(VectorSource(complaints_train_tidy$complaint))
# print(complaints_corpus1)
# 
# # inspect(complaints_corpus1[1:5])
# 
# complaints_corpus_tidy <- complaints_corpus1 |>
#   # tm_map(wordLengths(c(4, Inf))) |> 
#   tm_map(removePunctuation) |> 
#   tm_map(removeNumbers) |>
#   tm_map(stemDocument) |> 
#   tm_map(removeWords, stopwords()) |> 
#   tm_map(stripWhitespace)
# 
# print(complaints_corpus_tidy)
# inspect(complaints_corpus_tidy[1:5])
# 
# complaints_corpus_dtm1 <- DocumentTermMatrix(complaints_corpus_tidy)
# 
# inspect(complaints_corpus_dtm1)
```

### Variation 2

```{r}
complaints_corpus2 <- Corpus(VectorSource(complaints_train_tidy$complaint))
print(complaints_corpus2)

inspect(complaints_corpus2[1:5])

complaints_corpus_dtm2 <- DocumentTermMatrix(complaints_corpus2,
                                             control = list(
                                       stripWhitespace = TRUE,
                                       stopwords = TRUE,
                                       stemming = TRUE,
                                       removeNumbers = TRUE,
                                       wordLengths = c(4, Inf),
                                       removePunctuation = TRUE
                                       ))

inspect(complaints_corpus_dtm2)
```

```
<<DocumentTermMatrix (documents: 90975, terms: 52200)>>
Non-/sparse entries: 5778682/4743116318
Sparsity           : 100%
Maximal term length: 1961
Weighting          : term frequency (tf)
Sample             :
       Terms
Docs    account call card credit loan month payment receiv time told
  19650       4    8    0      4   51     1      13     21   10    0
  25455      97    0    0     15   81    12      33      5    4    0
  27671       3    1    0      1    4     0      13      3    4    0
  33614       5    2    7      6    8     2       1     20    5    0
  58908      24    1    0     27   62    63      54      3   19    0
  64243       0   14    0      0   84     7      11     11   23    2
  70389       1    1    2      4    6     3       3      2    0    0
  73540      16    5    0      9   64     4      44      6   11    0
  77518       0    0    0      0  854    16      46      0    0    2
  84551      58    0    0     12   70     6      38      0    8    0
```


### Variation 3

```{r}
# # count number of times word appears within each text
# complaints_words <- complaints_train_tidy |> 
#   # renumber the rows
#   mutate(complaint_id = as.factor(row_number())) |> 
#   unnest_tokens(word, complaint) |>
#   filter(!word %in% stop_words$word) |> 
#   relocate(word, .after = product)
# 
# complaints_words |> 
#   filter(str_length(word) > 15) #|> view()
# 
# skimr::skim(complaints_words)
# 
# complaints_count <- complaints_words |> 
#   count(complaint_id, product, word)
# 
# # tables
# complaints_count |> 
#   group_by(product) |> 
#   mutate(tot_n = sum(n),
#          percentage = n/tot_n) #|> view()
# 
# complaints_count |> 
#   group_by(product) |>
#   filter(n > 11)
# 
# complaints_count |> 
#   group_by(product) |> 
#   arrange(desc(n), .by_group = TRUE)
# 
# # create dtm
#   
# complaints_words_dtm <- complaints_count|>
#   cast_dtm(term = word, document = complaint_id, value = n)
# 
# inspect(complaints_words_dtm)
```

### Variation 4

```{r}
# complaints_stems <- complaints_words |>
#   mutate(stem = wordStem(word), .after = word)
# 
# 
# complaints_stems_count <- complaints_stems |> 
#   count(complaint_id, product, stem)
# 
# # create dtm
#   
# complaints_stems_dtm <- complaints_stems_count|>
#   cast_dtm(term = stem, document = complaint_id, value = n)
# 
# inspect(complaints_stems_dtm)
```

### Inspection

```{r}
# inspect(complaints_corpus_dtm1)
inspect(complaints_corpus_dtm2)
# inspect(complaints_words_dtm)
# inspect(complaints_stems_dtm)

# complaints_corpus_dtm1 <- removeSparseTerms(complaints_corpus_dtm1, 0.95)
# inspect(complaints_corpus_dtm1)
complaints_corpus_dtm2 <- removeSparseTerms(complaints_corpus_dtm2, 0.95)
inspect(complaints_corpus_dtm2)
# complaints_words_dtm <- removeSparseTerms(complaints_words_dtm, 0.95)
# inspect(complaints_words_dtm)
# complaints_stems_dtm <- removeSparseTerms(complaints_stems_dtm, 0.95)
# inspect(complaints_stems_dtm)
```

``` 
<<DocumentTermMatrix (documents: 90975, terms: 312)>>
Non-/sparse entries: 3526661/24857539
Sparsity           : 88%
Maximal term length: 10
Weighting          : term frequency (tf)
Sample             :
       Terms
Docs    account call card credit loan month payment receiv time told
  11002       1    0    0      1   70     5       8     11   15    1
  19650       4    8    0      4   51     1      13     21   10    0
  25455      97    0    0     15   81    12      33      5    4    0
  37990      20   32    0     11   47    11      10     31   12    9
  56802      20   10    0      0    6     9      34     28   14    8
  60769       0   27    6      5   47    12      11     30   17   18
  68621      11   15    0     10   67     8      25     21   23    8
  73540      16    5    0      9   64     4      44      6   11    0
  77518       0    0    0      0  854    16      46      0    0    2
  84551      58    0    0     12   70     6      38      0    8    0
```

### Add meta data

```{r}
create_matrix <- function(dtm, meta){
  complaints_matrix <- as.matrix(dtm)
  complaints_matrix
  
  model_matrix <- meta |> 
    cbind(complaints_matrix) |>
    as_tibble()
  
  return(model_matrix)
}

complaints_meta <- complaints_train |> 
  select(-complaint) |> 
  rename(product_value = product,
         company_dummy = company,
         state_dummy = state)

# corpus_model1 <- create_matrix(complaints_corpus_dtm1, complaints_meta)
corpus_model2 <- create_matrix(complaints_corpus_dtm2, complaints_meta)
# words_model <- create_matrix(complaints_words_dtm, complaints_meta)
# stems_model <- create_matrix(complaints_stems_dtm, complaints_meta)

# # count total number of words in each poem
# total_words <- complaints_words |> 
#   group_by(product) |> 
#   summarize(total = sum(n))
# 
# complaints_words <- left_join(complaints_words, total_words)
```

## Trash

```{r}
# # count number of times word appears within each text
# complaints_words <- complaints_train |> 
#   filter(!is.na(complaint)) |> 
#   select(product, complaint) |>
#   mutate(complaint = str_replace_all(complaint, "X{2,4}", ""),
#          "complaint_id" = row_number(), .after = product) |>
#   unnest_tokens(word, complaint) |>
#   filter(!word %in% stop_words$word) |> 
#   count(product, word, sort = TRUE)
# 
# # count total number of words in each poem
# total_words <- complaints_words |> 
#   group_by(product) |> 
#   summarize(total = sum(n))
# 
# complaints_words <- left_join(complaints_words, total_words)
```

```{r}
# # visualize frequency / total words in complaints
# ggplot(complaints_words_joined, aes(n/total, fill = product)) +
#   geom_histogram(show.legend = FALSE, bins = 5) +
#   facet_wrap(~product, ncol = 2, scales = "free_y")
```

```{r}
# # added column with term frequency
# freq_by_rank <- complaints_words |> 
#   group_by(product) |> 
#   mutate(rank = row_number(), 
#          `term frequency` = n/total)
# 
# # However, we’re not just interested in word frequency, as stop words (such as “a”) have the highest term frequency. Rather, we’re interested in tf-idf - those words in a document that are unique relative to the other documents being analyzed.
# 
# complaints_words <- complaints_words |>
#   bind_tf_idf(word, product, n)
# 
# # sort ascending
# complaints_words |>
#   arrange(tf_idf)
# 
# # sort descending
# complaints_words |>
#   arrange(desc(tf_idf))
```

```{r}

# complaints_words |>
#   arrange(desc(tf_idf)) |>
#   mutate(word = factor(word, levels = rev(unique(word)))) |> 
#   group_by(product) |> 
#   top_n(30) |> 
#   ungroup() |>
#   ggplot(aes(word, tf_idf, fill = product)) +
#   geom_col(show.legend = FALSE) +
#   labs(x = NULL, y = "tf-idf") +
#   facet_wrap(~product, ncol = 2, scales = "free") +
#   coord_flip()
# 
# 
# complaints_words |>
#   arrange(desc(tf)) |>
#   mutate(word = factor(word, levels = rev(unique(word)))) |> 
#   group_by(product) |> 
#   top_n(30) |> 
#   ungroup() |>
#   ggplot(aes(word, tf, fill = product)) +
#   geom_col(show.legend = FALSE) +
#   labs(x = NULL, y = "tf") +
#   facet_wrap(~product, ncol = 2, scales = "free") +
#   coord_flip()
```

```{r}
# 
# complaints_train_id <- complaints_train |> 
#   select(product, complaint) |>
#   mutate("complaint" = str_replace_all(complaint, "X{2,4}", ""),
#          "id_complaint" = row_number(), .after = product)
# 
# complaints_train_otpdpr <- complaints_train_id |> 
#   unnest_tokens(word, complaint) |> 
#   filter(!word %in% stop_words$word)
# 
# complaints_counts <- complaints_train_otpdpr |> 
#   count(word, product, sort = TRUE) |>
#   cast_dtm(document = product, term = word, value = n) |> 
#   tidy() |> 
#   rename(word = term, product = document, prod_count = count)
# 
# complaints_tot
# complaints_counts |> 
#   group_by(word) |> 
#   mutate(word_freq =  )
# 
# count_join <- complaints_counts |>
#   left_join(complaints_train_otpdpr, by = join_by(product, word)) |> 
#   unique()
# 
# complaints_joined <- count_join
# 
# # count_join |> 
# #   pivot_wider(names_from = word, values_from = prod_count)

```

#### Explore Frequency Matrix

```{r}
# complaints_modeling
# 
# glimpse(complaints_modeling |> select(1:25))
# skimr::skim(complaints_modeling)
```


#### Evaluate State-Complaint Correlation

```{r}
# install.packages("corrplot") 
# library(corrplot)  

# state_cor <- cor(complaints_modeling) 
# corrplot::corrplot(state_cor, tl.cex = 0.5)
```

# Modeling

## Iteration 1

```{r}
# complaints_modeling_small <- complaints_modeling
# complaints_modeling_big <- corpus_model2

# complaints_modeling <- corpus_model1
complaints_modeling <- corpus_model2
# complaints_modeling <- words_model
# complaints_modeling <- stems_model

# complaints_modeling <- complaints_modeling_big
```

### Step 1: Data Splitting with rsample

```{r}
# ensure the split is done the same if code is rerun
set.seed(1234)
# split data set into test and training
split_complaints <- initial_split(complaints_modeling, prop = 2/3) 
split_complaints

# split_complaints <- initial_split(complaints_counts, prop = 2/3) 
# split_complaints
```

```{r}
training_complaints <- training(split_complaints)
# head(training_complaints)
count(training_complaints, product_value)

testing_complaints <-testing(split_complaints)
# head(testing_complaints)
count(testing_complaints, product_value)
```

#### Step 1.2 Split training set into cross validation sets

```{r}
set.seed(1234)
vfold_complaints <- rsample::vfold_cv(data = training_complaints, v = 10)
vfold_complaints

pull(vfold_complaints, splits)
```

```{r}
# Explore one of the folds
first_fold <- vfold_complaints$splits[[1]]
head(as.data.frame(first_fold, data = "analysis")) # training set of this fold

head(as.data.frame(first_fold, data = "assessment")) # test set of this fold
```

### Step 2: Create recipe with recipe()

#### rpart

```{r}
complaints_recipe <- training_complaints |>
  recipe(product_value ~ .) |> 
  update_role(complaint_id, new_role = "comlaint id") |> 
  # Specify preprocessing steps 
  step_dummy(state_dummy, company_dummy, zip_code, one_hot = TRUE) |> 
  step_nzv(all_predictors()) |> 
  # step_corr(all_predictors())
complaints_recipe

summary(complaints_recipe)
```




#### Check the preprocessing

```{r}
prepped_rec <- prep(complaints_recipe, verbose = TRUE, retain = TRUE)

names(prepped_rec)
```

```{r}
preproc_train <- bake(prepped_rec, new_data = NULL)
glimpse(preproc_train)
```

##### Extract preprocessed testing data using bake()

```{r}
baked_test_complaints <- recipes::bake(prepped_rec, new_data = testing_complaints)
glimpse(baked_test_complaints)
```

Some of our levels were not previously seen in the training set!

-\> check differences between testing and training set

```{r}
traincompanies <- training_complaints |>
  distinct(company_dummy)
testcompanies <- testing_complaints |>
  distinct(company_dummy)

#get the number of companies that were different
dim(dplyr::setdiff(traincompanies, testcompanies))

#get the number of companies that overlapped
dim(dplyr::intersect(traincompanies, testcompanies))

complaints_modeling |> skimr::skim(company_dummy)


traincompanies <- training_complaints |>
  distinct(state_dummy)
testcompanies <- testing_complaints |>
  distinct(state_dummy)

#get the number of companies that were different
dim(dplyr::setdiff(traincompanies, testcompanies))

#get the number of companies that overlapped
dim(dplyr::intersect(traincompanies, testcompanies))
```

### Step 3: Specify model, engine, and mode (parsnip)

```{r}
complaints_model <- 
  # define Model (Classification And Regression Tree (CART))
  parsnip::decision_tree() |>
  #set engine (rpart)
  parsnip::set_engine("rpart") |>
  # set mode (classification)
  parsnip::set_mode("classification")

complaints_model
```

#### Specify hyperparameters to tune (tune())

### Step 4: Create workflow, add recipe, add model

```{r}
prod_comp_wflow <- workflows::workflow() |>
                   workflows::add_recipe(complaints_recipe) |>
                   workflows::add_model(complaints_model)

prod_comp_wflow
```

### Step 5.1 Fit workflow with cross validation

```{r}
prod_comp_wflow_fit <- parsnip::fit(prod_comp_wflow, data = training_complaints)

prod_comp_wflow_fit

```

#### Assessing the Model Fit

```{r}
wf_fit_comp <- prod_comp_wflow_fit |> 
  extract_fit_parsnip()
wf_fit_comp

wf_fit_comp$fit$variable.importance
```

#### Explore Variable importance using vip()

```{r}
prod_comp_wflow_fit |>
  pull_workflow_fit() |>  
  vip(num_features = 10)
```

### Step 5.2: Fit workflow with cross validation

```{r}
set.seed(122)
resample_fit <- tune::fit_resamples(prod_comp_wflow, vfold_complaints)

resample_fit



```

### Step 5.3: Fit workflow with tuning

```{r}
tune_model <- 
  # define Model (Classification And Regression Tree (CART))
  parsnip::decision_tree() |>
  #set engine (rpart)
  parsnip::set_engine("rpart") |>
  # set mode (classification)
  parsnip::set_mode("classification")

complaints_model

resample_fit <- tune::tune_grid(prod_comp_wflow_tune, resamples = vfold_complaints, grid = 4)
# 
# tune::collect_metrics(resample_fit)
# 
# tune::show_best(resample_fit, metric = "accuracy")
```

### Step 6: Get predictions

```{r}
pred_products <- predict(prod_comp_wflow_fit, new_data = training_complaints)
pred_products_test <- predict(prod_comp_wflow_fit, new_data = testing_complaints)

```



### Step 7: Use predictions to get performance metrics

```{r}
model_performance <- training_complaints |> 
 mutate(predictions = pred_products$.pred_class) |> 
 metrics(truth = product_value, estimate = predictions)

print(model_performance)

count(training_complaints, product_value)
count(pred_products, .pred_class)

predicted_and_truth <- bind_cols(training_complaints, 
        predicted_products = pull(pred_products, .pred_class))

head(predicted_and_truth)

filter(predicted_and_truth, product_value != predicted_products)


model_performance_test <- testing_complaints |> 
  mutate(predictions = pred_products_test$.pred_class, .after = product_value) |> 
  metrics(truth = product_value, estimate = predictions)

print(model_performance_test)

count(testing_complaints, product_value)
count(pred_products_test, .pred_class)

predicted_and_truth <- bind_cols(training_complaints, 
        predicted_products = pull(pred_products, .pred_class))

head(predicted_and_truth)

filter(predicted_and_truth, product_value != predicted_products)


```

### Step 7.1: Quantifying Model Performance (unnecessary)

Here we see the RMSE in addition to the RSQ or the R squared value

```{r}
yardstick::metrics(wf_fitted_values, 
                   truth = value, estimate = .fitted)

yardstick::rmse(wf_fitted_values, 
               truth = value, estimate = .fitted)
```

### Step: 7.2: Get performance metrics

```{r}
collect_metrics(resample_fit)

show_best(resample_fit, metrics = "accuracy")
```



## Workshop

```{r}

complaints_modeling <- complaints_train_tidy |> 
  rename(product_value = product,
         company_dummy = company,
         state_dummy = state) |> 
  select(-c(company_dummy, state_dummy, zip_code)) |> 
  mutate(complaint_id = as.factor(row_number()))
  

# ensure the split is done the same if code is rerun
set.seed(1234)
# split data set into test and training
split_complaints <- initial_split(complaints_modeling, prop = 2/3) 
split_complaints

# split_complaints <- initial_split(complaints_counts, prop = 2/3) 
# split_complaints
```

```{r}
training_complaints <- training(split_complaints)
# head(training_complaints)
count(training_complaints, product_value)

testing_complaints <-testing(split_complaints)
# head(testing_complaints)
count(testing_complaints, product_value)


# library(textrecipes)

token_recipe <- training_complaints |> 
  recipe(product_value ~ .) |> 
  update_role(complaint_id, new_role = "comlaint id") |> 
  # Specify preprocessing steps 
  # step_dummy(state_dummy, company_dummy, zip_code, one_hot = TRUE) |> 
  step_tokenize(complaint) |> 
  step_stem(complaint) |> 
  step_stopwords(complaint) |> 
  step_tokenfilter(complaint, max_tokens = 50) |> 
  step_tfidf(complaint) |> 
  # step_nzv(all_predictors()) |> 
  # step_corr(all_predictors())

summary(token_recipe)
```

### Part

```{r}

token_obj <- token_recipe |> prep(verbose = TRUE, retain = TRUE)

names(prepped_rec)

str(bake(token_obj, training_complaints))
```

```{r}
preproc_train <- bake(token_obj, new_data = NULL)
glimpse(preproc_train)
```

##### Extract preprocessed testing data using bake()

```{r}
baked_test_complaints <- bake(prepped_rec, new_data = testing_complaints)
glimpse(baked_test_complaints)
```






















#### Save predicted outcome values

```{r}

```

#### Visualizing Model Performance

```{r}
# *wf_fitted_values* %>% 
#   ggplot(aes(x =  value, y = .fitted)) + 
#   geom_point() + 
#   xlab("actual outcome values") + 
#   ylab("predicted outcome values")
```

```{r}
pred_products <- predict(prod_comp_wflow_fit, new_data = training_complaints)


yardstick::accuracy(training_complaints, 
                truth = product_value, estimate = pred_products$.pred_class)

count(training_complaints, product_value)
count(pred_products, .pred_class)

predicted_and_truth <- bind_cols(training_complaints, 
        predicted_products = pull(pred_products, .pred_class))

head(predicted_and_truth)

filter(predicted_and_truth, product_value != predicted_products)
```

```{r}
set.seed(122)
resample_fit <- tune::fit_resamples(prod_comp_wflow, vfold_complaints)

resample_fit

collect_metrics(resample_fit)
```

```{r}
final_workflow <- finalize_workflow(my_wf, my_best_model)
final_model <- fit(final_workflow, training_complaints)
my_results <- predict(final_model, testing_complaints)

my_results |> knitr::kable( caption = "My Predictions")
```

### Criteria

1.  Does the submission build a machine learning algorithm to classify consumer complaints?

2.  Do the authors describe what they expect the out of sample error to be and estimate the error appropriately with cross-validation?

This is NOT giving anything away in terms of the grade. A few people have submitted final projects which FULLY MET the stated grading criteria. So I gave them full marks.

But in these projects, the individuals did not take the extra step of fitting their final model to the test data set.

If we are using the Tidymodels framework, and you have earlier chosen your best model *my_best_model* $$see **?show_best** for options$$, then with all due love and respect, your final code block generically might look like this:

```{r}

```

### Frequency matrix

From the exploring of the data, no correlation between the different variables were visible. The most promising way to design the model is to create a matrix with how frequent a given word appears for a given complaint and use it as features for a machine learning algorithm. In the following the matrix is generated.

```{r}
# reduce sample size
complaints_train <- complaints_train |> 
  sample_n(100)


complaints_train_id <- complaints_train |> 
  select(product_value, complaint) |>
  mutate("complaint" = str_replace_all(complaint, "X{2,4}", ""),
         "id_complaint" = row_number(), .after = product_value)

complaints_train_otpdpr <- complaints_train_id |> 
  unnest_tokens(word, complaint) |> 
  filter(!word %in% stop_words$word)

complaints_counts <- complaints_train_otpdpr |> 
  count(word, .by = product_value, sort = TRUE) |>
  rename(product_value = .by) |>
  cast_dtm(document = product_value, term = word, value = n) |> 
  tidy() |> 
  rename(word = term, product_value = document, prod_count = count)

count_join <- complaints_counts |>
  left_join(complaints_train_otpdpr, by = join_by(product_value, word)) |> 
  unique()

complaints_simlpe <- complaints_train |>
  select(!c(complaint, zip_code, submitted_via)) |> 
  mutate("id_complaint" = row_number(), .after = product_value)
  
complaints_joined <- count_join |>
  left_join(complaints_simlpe, by = join_by(id_complaint, product_value))

complaints_joined |>
  pivot_wider(names_from = word, values_from = prod_count)
```

### Step 1: Data Splitting with rsample

```{r}
# ensure the split is done the same if code is rerun
set.seed(1234)
# split data set into test and training
split_complaints <- initial_split(complaints_joined, prop = 2/3) 
split_complaints

# split_complaints <- initial_split(complaints_counts, prop = 2/3) 
# split_complaints
```

```{r}
training_complaints <- training(split_complaints)
head(training_complaints)
count(training_complaints, product_value)

testing_complaints <-testing(split_complaints)
head(testing_complaints)
count(testing_complaints, product_value)
```

```{r}
set.seed(1234)
vfold_complaints <- rsample::vfold_cv(data = training_complaints, v = 4)
vfold_complaints

pull(vfold_complaints, splits)
```

```{r}
# Explore one of the folds
first_fold <-vfold_complaints$splits[[1]]
head(as.data.frame(first_fold, data = "analysis")) # training set of this fold

head(as.data.frame(first_fold, data = "assessment")) # test set of this fold
```

### Step 2: Create recipe with recipe()

```{r}
complaints_recipe <- training_complaints |>
  recipe(product_value ~ count + company + state) 

complaints_recipe

summary(complaints_recipe)
```

### Step 3: Specify model, engine, and mode (parsnip)

```{r}
complaints_model <- 
  # define Model (Classification And Regression Tree (CART))
  parsnip::decision_tree() |>
  # set mode (classification)
  parsnip::set_mode("classification") |>
  #set engine (rpart)
  parsnip::set_engine("rpart")

complaints_model

```

#### Specify hyperparameters to tune (tune())

### Step 4: Create workflow, add recipe, add model

```{r}
prod_comp_wflow <-workflows::workflow() |>
           workflows::add_recipe(complaints_recipe) |>
           workflows::add_model(complaints_model)

prod_comp_wflow
```

### Step 5.1 Fit workflow with cross validation

```{r}
prod_comp_wflow_fit <- parsnip::fit(prod_comp_wflow, data = training_complaints)

prod_comp_wflow_fit

# store fit
wf_fit_comp <- prod_comp_wflow_fit |> 
  pull_workflow_fit()

wf_fit_comp$fit$variable.importance
```

### Step 5.2: Fit workflow with cross validation

```{r}
set.seed(122)
resample_fit <- tune::fit_resamples(prod_comp_wflow, vfold_complaints)
```

### Step 5.3: Fir workflow with tuning

```{r}
reasmple_fit <-tune::tune_grid(prod_comp_wflow_tune, resamples = vfold_complaints, grid = 4)

tune::collect_metrics(resample_fit)

tune::show_best(resample_fit, metric = "accuracy")
```

### Step 6: Get predictions

```{r}
pred_products <- predict(prof_comp_wflow_fit, new_data = training_complaints)


yardstick::accuracy(training_complaints, 
                truth = products, estimate = pred_products$.pred_class)

count(training_complaints, Species)
count(pred_products, .pred_class)

predicted_and_truth <- bind_cols(training_complaints, 
        predicted_products = pull(pred_products, .pred_class))

head(predicted_and_truth)

filter(predicted_and_truth, products != predicted_products)
```

```{r}
set.seed(122)
resample_fit <- tune::fit_resamples(prod_comp_wflow, vfold_complaints)

resample_fit

collect_metrics(resample_fit)
```

```{r}
final_workflow <- finalize_workflow(my_wf, my_best_model)
final_model <- fit(final_workflow, my_training_data)
my_results <- predict(final_model, my_testing_data )

my_results |> knitr::kable( caption = "My Predictions")
```

### Criteria

1.  Does the submission build a machine learning algorithm to classify consumer complaints?

2.  Do the authors describe what they expect the out of sample error to be and estimate the error appropriately with cross-validation?

This is NOT giving anything away in terms of the grade. A few people have submitted final projects which FULLY MET the stated grading criteria. So I gave them full marks.

But in these projects, the individuals did not take the extra step of fitting their final model to the test data set.

If we are using the Tidymodels framework, and you have earlier chosen your best model *my_best_model* $$see **?show_best** for options$$, then with all due love and respect, your final code block generically might look like this:

```{r}

```
