---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup_chunks, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "images/",
  out.width = "100%",
  message = FALSE,
  warning = FALSE
)
```

## Load Packages

```{r}
library(readr)
library(here)
library(tidyverse)
library(tidytext)
library(tm)
library(rsample)
library(recipes)
library(rpart)
library(tune)
library(vip)
library(workflows)
```

## Load data sets

```{r}
complaints_train <- read_csv(here::here("data", "raw_data", "data_complaints_train.csv")) |> 
  janitor::clean_names() |> 
  select(-submitted_via) |> 
  mutate(complaint_id = as.factor(row_number()), .before = everything(),
         product = as.factor(product)) |> 
  rename(complaint = consumer_complaint_narrative)
complaints_train

complaints_test <- read_csv(here::here("data", "raw_data", "data_complaints_test.csv")) |>
  janitor::clean_names() |> 
  select(-submitted_via)
complaints_test

# save(complaints_train, complaints_test, file = here::here("data", "tidy_data", "project_5.rda"))
```

## Explore data

```{r}
head(complaints_train)
glimpse(complaints_train)
head(complaints_test)
glimpse(complaints_test)

skimr::skim(complaints_train)
skimr::skim(complaints_test)

# complaints_train |> distinct(state) |> view()
```

For the training data we have initially 90'975 complaints (rows) and 6 total variables (columns) where no values are missing. The variable state has 62 unique values which include all the 50 US states and other territories outside the Union as Puerto Rico (PR) or Armed forces Pacific (AP).

#### Term frequency

From the exploring of the data, no correlation between the different variables were visible. The most promising way to design the model is to create a matrix with how frequent a given word appears for a given complaint and use it as features for a machine learning algorithm. In the following the matrix is generated.

```{r}
# reduce sample size
complaints_train <- complaints_train |>
  sample_n(200)

```

```{r}
# count number of times word appears within each text
complaints_words <- complaints_train |> 
  mutate(complaint = str_replace_all(complaint, "X{2,4}", "")) |>
  mutate(complaint_id = as.factor(row_number())) |> 
  unnest_tokens(word, complaint) |>
  filter(!word %in% stop_words$word) |> 
  relocate(word, .after = product)

complaints_count <- complaints_words |> 
  count(complaint_id, product, word)

complaints_count |> 
  group_by(product) |> 
  arrange(desc(n), .by_group = TRUE) |> view()
  
complaints_dtm <- complaints_count|>
  cast_dtm(term = word, document = complaint_id, value = n)

inspect(complaints_dtm)

complaints_matrix <- as.matrix(complaints_dtm)
complaints_matrix

complaints_modeling <- complaints_train |> 
  select(-complaint) |> 
  rename(product_value = product,
         company_dummy = company) |> 
  cbind(complaints_matrix) |>
  as_tibble()

# # count total number of words in each poem
# total_words <- complaints_words |> 
#   group_by(product) |> 
#   summarize(total = sum(n))
# 
# complaints_words <- left_join(complaints_words, total_words)
```

```{r}
# # count number of times word appears within each text
# complaints_words <- complaints_train |> 
#   filter(!is.na(complaint)) |> 
#   select(product, complaint) |>
#   mutate(complaint = str_replace_all(complaint, "X{2,4}", ""),
#          "complaint_id" = row_number(), .after = product) |>
#   unnest_tokens(word, complaint) |>
#   filter(!word %in% stop_words$word) |> 
#   count(product, word, sort = TRUE)
# 
# # count total number of words in each poem
# total_words <- complaints_words |> 
#   group_by(product) |> 
#   summarize(total = sum(n))
# 
# complaints_words <- left_join(complaints_words, total_words)
```

```{r}
# # visualize frequency / total words in complaints
# ggplot(complaints_words_joined, aes(n/total, fill = product)) +
#   geom_histogram(show.legend = FALSE, bins = 5) +
#   facet_wrap(~product, ncol = 2, scales = "free_y")
```

```{r}
# # added column with term frequency
# freq_by_rank <- complaints_words |> 
#   group_by(product) |> 
#   mutate(rank = row_number(), 
#          `term frequency` = n/total)
# 
# # However, we’re not just interested in word frequency, as stop words (such as “a”) have the highest term frequency. Rather, we’re interested in tf-idf - those words in a document that are unique relative to the other documents being analyzed.
# 
# complaints_words <- complaints_words |>
#   bind_tf_idf(word, product, n)
# 
# # sort ascending
# complaints_words |>
#   arrange(tf_idf)
# 
# # sort descending
# complaints_words |>
#   arrange(desc(tf_idf))
```

```{r}

# complaints_words |>
#   arrange(desc(tf_idf)) |>
#   mutate(word = factor(word, levels = rev(unique(word)))) |> 
#   group_by(product) |> 
#   top_n(30) |> 
#   ungroup() |>
#   ggplot(aes(word, tf_idf, fill = product)) +
#   geom_col(show.legend = FALSE) +
#   labs(x = NULL, y = "tf-idf") +
#   facet_wrap(~product, ncol = 2, scales = "free") +
#   coord_flip()
# 
# 
# complaints_words |>
#   arrange(desc(tf)) |>
#   mutate(word = factor(word, levels = rev(unique(word)))) |> 
#   group_by(product) |> 
#   top_n(30) |> 
#   ungroup() |>
#   ggplot(aes(word, tf, fill = product)) +
#   geom_col(show.legend = FALSE) +
#   labs(x = NULL, y = "tf") +
#   facet_wrap(~product, ncol = 2, scales = "free") +
#   coord_flip()
```

```{r}
# 
# complaints_train_id <- complaints_train |> 
#   select(product, complaint) |>
#   mutate("complaint" = str_replace_all(complaint, "X{2,4}", ""),
#          "id_complaint" = row_number(), .after = product)
# 
# complaints_train_otpdpr <- complaints_train_id |> 
#   unnest_tokens(word, complaint) |> 
#   filter(!word %in% stop_words$word)
# 
# complaints_counts <- complaints_train_otpdpr |> 
#   count(word, product, sort = TRUE) |>
#   cast_dtm(document = product, term = word, value = n) |> 
#   tidy() |> 
#   rename(word = term, product = document, prod_count = count)
# 
# complaints_tot
# complaints_counts |> 
#   group_by(word) |> 
#   mutate(word_freq =  )
# 
# count_join <- complaints_counts |>
#   left_join(complaints_train_otpdpr, by = join_by(product, word)) |> 
#   unique()
# 
# complaints_joined <- count_join
# 
# # count_join |> 
# #   pivot_wider(names_from = word, values_from = prod_count)

```

#### Explore Frequency Matrix

```{r}
complaints_modeling

glimpse(complaints_modeling |> select(1:25))
# skimr::skim(complaints_modeling)
```


#### Evaluate State-Complaint Correlation

```{r}
# install.packages("corrplot") 
# library(corrplot)  

# state_cor <- cor(complaints_modeling) 
# corrplot::corrplot(state_cor, tl.cex = 0.5)
```

No correlation can be determined from the corrplot() function.

### Step 1: Data Splitting with rsample

```{r}
# ensure the split is done the same if code is rerun
set.seed(1234)
# split data set into test and training
split_complaints <- initial_split(complaints_modeling, prop = 2/3) 
split_complaints

# split_complaints <- initial_split(complaints_counts, prop = 2/3) 
# split_complaints
```

```{r}
training_complaints <- training(split_complaints)
head(training_complaints)
count(training_complaints, product_value)

testing_complaints <-testing(split_complaints)
head(testing_complaints)
count(testing_complaints, product_value)
```

```{r}
set.seed(1234)
vfold_complaints <- rsample::vfold_cv(data = training_complaints, v = 4)
vfold_complaints

pull(vfold_complaints, splits)
```

```{r}
# Explore one of the folds
first_fold <- vfold_complaints$splits[[1]]
head(as.data.frame(first_fold, data = "analysis")) # training set of this fold

head(as.data.frame(first_fold, data = "assessment")) # test set of this fold
```

### Step 2: Create recipe with recipe()

```{r}
complaints_recipe <- training_complaints |>
  recipe(product_value ~ .) |> 
  update_role(complaint_id, new_role = "comlaint id") |> 
  # Specify preprocessing steps 
  step_dummy(state, company_dummy, zip_code, one_hot = TRUE) |> 
  step_corr(all_predictors()) |> 
  step_nzv(all_predictors())

complaints_recipe

summary(complaints_recipe)
```

#### Check the preprocessing

```{r}
prepped_rec <- prep(complaints_recipe, verbose = TRUE, retain = TRUE)

names(prepped_rec)
```

```{r}
preproc_train <- bake(prepped_rec, new_data = NULL)
glimpse(preproc_train)
```

#### Extract preprocessed testing data using bake()

```{r}
baked_test_complaints <- recipes::bake(prepped_rec, new_data = testing_complaints)
glimpse(baked_test_complaints)
```

Some of our levels were not previously seen in the training set! (company_Not.a.company???)

-\> check differences between testing and training set

```{r}
traincompanies <- training_complaints |>
  distinct(company)
testcompanies <- ttesting_complaints |>
  distinct(company)

#get the number of companies that were different
dim(dplyr::setdiff(traincompanies, testcompanies))

#get the number of companies that overlapped
dim(dplyr::intersect(traincompanies, testcompanies))
```

## Itration 2

### Fix dataset

```{r}
pm %<>%
  mutate(city = case_when(city == "Not in a city" ~ "Not in a city",
                          city != "Not in a city" ~ "In a city"))
glimpse(pm)
```

### Step 1: Data Splitting with rsample

```{r}
# ensure the split is done the same if code is rerun
set.seed(1234)
# split data set into test and training
pm_split <- rsample::initial_split(data = pm, prop = 2/3)
pm_split

train_pm <- training(pm_split)
test_pm <-testing(pm_split)
```

### Step 2: Create recipe with recipe()

```{r}
novel_rec <- train_pm  |> 
  recipes::recipe(value ~ .) |>
    update_role(id, new_role = "id variable") |>
    update_role("fips", new_role = "county id") |>
    step_dummy(state, county, city, zcta, one_hot = TRUE) |>
    step_corr(all_numeric()) |>
    step_nzv(all_numeric()) 

novel_rec
```

#### Check the preprocessing

```{r}
prepped_rec <- prep(novel_rec, verbose = TRUE, retain = TRUE)
```

```{r}
preproc_train <- bake(prepped_rec, new_data = NULL) 
glimpse(preproc_train)
```

We now only have *38* variables instead of 50 and no longer categorical variables.

#### Extract preprocessed testing data using bake()

```{r}
baked_test_pm <- recipes::bake(prepped_rec, new_data = testing_complaints)
glimpse(baked_test_pm)
```

Great, now we no longer have NA values!

### Step 3: Specify model, engine, and mode (parsnip)

```{r}
complaints_model <- 
  # define Model (Classification And Regression Tree (CART))
  parsnip::decision_tree() |>
  #set engine (rpart)
  parsnip::set_engine("rpart") |>
  # set mode (classification)
  parsnip::set_mode("classification")

complaints_model

```

#### Specify hyperparameters to tune (tune())

### Step 4: Create workflow, add recipe, add model

```{r}
prod_comp_wflow <- workflows::workflow() |>
                   workflows::add_recipe(complaints_recipe) |>
                   workflows::add_model(complaints_model)

prod_comp_wflow
```

### Step 5.1 Fit workflow with cross validation

```{r}
prod_comp_wflow_fit <- parsnip::fit(prod_comp_wflow, data = training_complaints)

prod_comp_wflow_fit

```

#### Assessing the Model Fit

```{r}
wf_fit_comp <- prod_comp_wflow_fit |> 
  extract_fit_parsnip() |> 
  broom::tidy()
wf_fit_comp

wf_fit_comp$fit$variable.importance
```

### Step 5.2: Fit workflow with cross validation

```{r}
set.seed(122)
resample_fit <- tune::fit_resamples(prod_comp_wflow, vfold_complaints)
```

### Step 5.3: Fit workflow with tuning

```{r}
reasmple_fit <-tune::tune_grid(prod_comp_wflow_tune, resamples = vfold_complaints, grid = 4)

tune::collect_metrics(resample_fit)

tune::show_best(resample_fit, metric = "accuracy")
```

### Step 6: Get predictions

```{r}

```

#### Save predicted outcome values

```{r}

```


#### Visualizing Model Performance

```{r}
# *wf_fitted_values* %>% 
#   ggplot(aes(x =  value, y = .fitted)) + 
#   geom_point() + 
#   xlab("actual outcome values") + 
#   ylab("predicted outcome values")
```

```{r}
pred_products <- predict(prod_comp_wflow_fit, new_data = training_complaints)


yardstick::accuracy(training_complaints, 
                truth = product_value, estimate = pred_products$.pred_class)

count(training_complaints, product_value)
count(pred_products, .pred_class)

predicted_and_truth <- bind_cols(training_complaints, 
        predicted_products = pull(pred_products, .pred_class))

head(predicted_and_truth)

filter(predicted_and_truth, product_value != predicted_products)
```

```{r}
set.seed(122)
resample_fit <- tune::fit_resamples(prod_comp_wflow, vfold_complaints)

resample_fit

collect_metrics(resample_fit)
```

```{r}
final_workflow <- finalize_workflow(my_wf, my_best_model)
final_model <- fit(final_workflow, my_training_data)
my_results <- predict(final_model, my_testing_data )

my_results |> knitr::kable( caption = "My Predictions")
```

### Criteria

1.  Does the submission build a machine learning algorithm to classify consumer complaints?

2.  Do the authors describe what they expect the out of sample error to be and estimate the error appropriately with cross-validation?

This is NOT giving anything away in terms of the grade. A few people have submitted final projects which FULLY MET the stated grading criteria. So I gave them full marks.

But in these projects, the individuals did not take the extra step of fitting their final model to the test data set.

If we are using the Tidymodels framework, and you have earlier chosen your best model *my_best_model* $$see **?show_best** for options$$, then with all due love and respect, your final code block generically might look like this:

```{r}

```

### Frequency matrix

From the exploring of the data, no correlation between the different variables were visible. The most promising way to design the model is to create a matrix with how frequent a given word appears for a given complaint and use it as features for a machine learning algorithm. In the following the matrix is generated.

```{r}
# reduce sample size
complaints_train <- complaints_train |> 
  sample_n(100)


complaints_train_id <- complaints_train |> 
  select(product_value, complaint) |>
  mutate("complaint" = str_replace_all(complaint, "X{2,4}", ""),
         "id_complaint" = row_number(), .after = product_value)

complaints_train_otpdpr <- complaints_train_id |> 
  unnest_tokens(word, complaint) |> 
  filter(!word %in% stop_words$word)

complaints_counts <- complaints_train_otpdpr |> 
  count(word, .by = product_value, sort = TRUE) |>
  rename(product_value = .by) |>
  cast_dtm(document = product_value, term = word, value = n) |> 
  tidy() |> 
  rename(word = term, product_value = document, prod_count = count)

count_join <- complaints_counts |>
  left_join(complaints_train_otpdpr, by = join_by(product_value, word)) |> 
  unique()

complaints_simlpe <- complaints_train |>
  select(!c(complaint, zip_code, submitted_via)) |> 
  mutate("id_complaint" = row_number(), .after = product_value)
  
complaints_joined <- count_join |>
  left_join(complaints_simlpe, by = join_by(id_complaint, product_value))

complaints_joined |>
  pivot_wider(names_from = word, values_from = prod_count)
```

### Step 1: Data Splitting with rsample

```{r}
# ensure the split is done the same if code is rerun
set.seed(1234)
# split data set into test and training
split_complaints <- initial_split(complaints_joined, prop = 2/3) 
split_complaints

# split_complaints <- initial_split(complaints_counts, prop = 2/3) 
# split_complaints
```

```{r}
training_complaints <- training(split_complaints)
head(training_complaints)
count(training_complaints, product_value)

testing_complaints <-testing(split_complaints)
head(testing_complaints)
count(testing_complaints, product_value)
```

```{r}
set.seed(1234)
vfold_complaints <- rsample::vfold_cv(data = training_complaints, v = 4)
vfold_complaints

pull(vfold_complaints, splits)
```

```{r}
# Explore one of the folds
first_fold <-vfold_complaints$splits[[1]]
head(as.data.frame(first_fold, data = "analysis")) # training set of this fold

head(as.data.frame(first_fold, data = "assessment")) # test set of this fold
```

### Step 2: Create recipe with recipe()

```{r}
complaints_recipe <- training_complaints |>
  recipe(product_value ~ count + company + state) 

complaints_recipe

summary(complaints_recipe)
```

### Step 3: Specify model, engine, and mode (parsnip)

```{r}
complaints_model <- 
  # define Model (Classification And Regression Tree (CART))
  parsnip::decision_tree() |>
  # set mode (classification)
  parsnip::set_mode("classification") |>
  #set engine (rpart)
  parsnip::set_engine("rpart")

complaints_model

```

#### Specify hyperparameters to tune (tune())

### Step 4: Create workflow, add recipe, add model

```{r}
prod_comp_wflow <-workflows::workflow() |>
           workflows::add_recipe(complaints_recipe) |>
           workflows::add_model(complaints_model)

prod_comp_wflow
```

### Step 5.1 Fit workflow with cross validation

```{r}
prod_comp_wflow_fit <- parsnip::fit(prod_comp_wflow, data = training_complaints)

prod_comp_wflow_fit

# store fit
wf_fit_comp <- prod_comp_wflow_fit |> 
  pull_workflow_fit()

wf_fit_comp$fit$variable.importance
```

### Step 5.2: Fit workflow with cross validation

```{r}
set.seed(122)
resample_fit <- tune::fit_resamples(prod_comp_wflow, vfold_complaints)
```

### Step 5.3: Fir workflow with tuning

```{r}
reasmple_fit <-tune::tune_grid(prod_comp_wflow_tune, resamples = vfold_complaints, grid = 4)

tune::collect_metrics(resample_fit)

tune::show_best(resample_fit, metric = "accuracy")
```

### Step 6: Get predictions

```{r}
pred_products <- predict(prof_comp_wflow_fit, new_data = training_complaints)


yardstick::accuracy(training_complaints, 
                truth = products, estimate = pred_products$.pred_class)

count(training_complaints, Species)
count(pred_products, .pred_class)

predicted_and_truth <- bind_cols(training_complaints, 
        predicted_products = pull(pred_products, .pred_class))

head(predicted_and_truth)

filter(predicted_and_truth, products != predicted_products)
```

```{r}
set.seed(122)
resample_fit <- tune::fit_resamples(prod_comp_wflow, vfold_complaints)

resample_fit

collect_metrics(resample_fit)
```

```{r}
final_workflow <- finalize_workflow(my_wf, my_best_model)
final_model <- fit(final_workflow, my_training_data)
my_results <- predict(final_model, my_testing_data )

my_results |> knitr::kable( caption = "My Predictions")
```

### Criteria

1.  Does the submission build a machine learning algorithm to classify consumer complaints?

2.  Do the authors describe what they expect the out of sample error to be and estimate the error appropriately with cross-validation?

This is NOT giving anything away in terms of the grade. A few people have submitted final projects which FULLY MET the stated grading criteria. So I gave them full marks.

But in these projects, the individuals did not take the extra step of fitting their final model to the test data set.

If we are using the Tidymodels framework, and you have earlier chosen your best model *my_best_model* $$see **?show_best** for options$$, then with all due love and respect, your final code block generically might look like this:

```{r}

```
